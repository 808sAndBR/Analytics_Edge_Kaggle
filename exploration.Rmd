---
title: "Kaggle"
author: "Scott Brenstuhl"
date: "May 29, 2016"
output: html_document
---


```{r, echo = FALSE}
# setwd("/Users/scottbrenstuhl/Projects/AnalyticsEdge/Analytics_Edge_Kaggle/")

library(caret)
library(doMC)
library(dplyr)
library(ggplot2)
library(mice)
library(randomForest)
library(readr)
library(rpart)
library(rpart.plot)

registerDoMC(cores = 4)

training <- read.csv("data/train2016.csv", na.strings = c("NA",""))
testing <- read.csv("data/test2016.csv", na.strings = c("NA",""))
```


# Exploration
```{r, echo=FALSE}
str(training)

# Commented out to avoid Rmd error
#View(head(training,10))

prop.table(table(complete.cases(training)))

# No overlap in user_ids to exploit 
table(testing$USER_ID %in% training$USER_ID)
```

Very low percent of respondednts answered all questions so I will probably need
to figure out some good impution strategies.


```{r, echo=FALSE}

CART <- rpart(Party ~.-USER_ID, data = training, method = "class")
prp(CART)

CART_pred <- predict(CART, newdata = testing, type = "class")

CART_submission <- data.frame(USER_ID = testing$USER_ID, 
                              Predictions = CART_pred)

write.csv(CART_submission, "submissions/BasicCART.csv", row.names=FALSE)

```

```{r, echo=FALSE}
# Make data sets considering NA a choice of no response

removeNA <- function(x){
    if(is.factor(x) & sum(is.na(x)) > 0){
        factor(as.character(x), levels = c(levels(x), "noResponse"))
    }else{
            x
        }
    }

trainNoAns <- as.data.frame(lapply(training, removeNA))
testNoAns <- as.data.frame(lapply(testing, removeNA))


trainNoAns[,-2][is.na(trainNoAns[,-2])] <- "noResponse"
testNoAns[,-2][is.na(testNoAns[,-2])] <- "noResponse"

avg_YOB <-round(mean(na.omit(trainNoAns$YOB)))

trainNoAns$YOB[is.na(trainNoAns$YOB)] <- avg_YOB
testNoAns$YOB[is.na(testNoAns$YOB)] <- avg_YOB

table(trainNoAns$YOB)
```

I should make 2039 be 1939

```{r, echo=FALSE}
noRespCART <- rpart(Party ~.-USER_ID, data = training, method = "class")

# These are the same so no point in predicting off this one
prp(noRespCART)
prp(CART)


# Lets try random forest instead though since we got rid of those pesky NAs
set.seed(1809)
noResp_RF <-train(Party ~.-USER_ID, data = trainNoAns, method = "rf",
                trControl = trainControl(method="cv", number=5),
                prox=TRUE, allowParallel=TRUE)

print(noResp_RF)
print(noResp_RF$finalModel)

varImp(noResp_RF)

noResp_RF_pred <- predict.train(noResp_RF, newdata = testNoAns)

noResp_submission <- data.frame(USER_ID = testing$USER_ID, 
                              Predictions = noResp_RF_pred)

write.csv(noResp_submission, "submissions/NoResp_RF.csv", row.names=FALSE)

```


```{r, echo = FALSE}

percent_missing <- function(column){
        col_table <- prop.table(table(column, useNA = "always"))
        as.numeric(col_table[is.na(names(col_table))])
    }

ans_missing <- lapply(training, percent_missing) 
ans_missing <- data_frame(qustion = names(ans_missing),
                          perc_missing = as.numeric(ans_missing))


qplot(qustion, y=perc_missing, data = subset(ans_missing, perc_missing >.45 ))
qplot(qustion, y=perc_missing, data = subset(ans_missing, perc_missing <.30 ))
qplot(qustion, y=perc_missing, data = subset(ans_missing, perc_missing >.30 &
                                                 perc_missing <.45 ))
```

Q124742 is missing a lot more responses than the rest so it will probably be
best to remove it.

Only the non-question (demagraphics) have less than 30% missing which isn't too
surprising.

```{r, echo = FALSE}
# Not so helpful but want to remember it for now
#md.pattern(training)

# This might be better to do after deciding which columns to use?
missing_perc <- apply(is.na(training),1,function(x) sum(x)/108)
table(round(missing_perc, digits = 1))
prop.table(table(missing_perc >.5))

dim(training)

```


```{r echo = FALSE}
training$Training <- 1
testing$Training <- 0

all_data <- rbind(training[names(training) != "Party"], testing)

# Looks like it worked
dim(training)
dim(testing)
dim(all_data)
summary(all_data)

imputed_all <- complete(mice(all_data[names(all_data) != 
                                          c("USER_ID", "Training")]))

write.csv(imputed_all, "data/imputed_all.csv", row.names=FALSE)
dim(training)

training$Training <- NULL
testing$Training <- NULL

imp_train <- cbind(USER_ID = training$USER_ID, 
                   Party = training$Party, 
                   imputed_all[1:5568,])
imp_test <- cbind(USER_ID = testing$USER_ID, imputed_all[5569:nrow(imputed_all),])

```

```{r, echo=FALSE}
imp_CART <- rpart(Party ~.-USER_ID, data = imp_train, method = "class")

# still the same but people get sorted better with data imputed 
prp(CART)
prp(imp_CART)

imp_CART_pred <- predict(imp_CART, newdata = testing, type = "class")

imp_CART_submission <- data.frame(USER_ID = testing$USER_ID, 
                              Predictions = imp_CART_pred)

write.csv(imp_CART_submission, "submissions/ImpCART.csv", row.names=FALSE)

```

```{r, echo = FALSE}

set.seed(1809)
imp_RF <-train(Party ~.-USER_ID, data = imp_train, method = "rf",
                trControl = trainControl(method="cv", number=5),
                prox=TRUE, allowParallel=TRUE)

print(imp_RF)
print(imp_RF$finalModel)

imp_RF_pred <- predict.train(imp_RF, newdata = imp_test)

impRF_submission <- data.frame(USER_ID = testing$USER_ID, 
                              Predictions = imp_RF_pred)

write.csv(impRF_submission, "submissions/Imp_RF.csv", row.names=FALSE)

```


```{r, echo = FALSE}
# I want to replace all NA with 0 Positive with 1 and negative with -1

#since the first facor level is No when yes/no lets make it negative
sapply(training, function(x) levels(x)[1])

train_num <- training
test_num <- testing

# Change strings to numbers then changes NA to 0
numericizer <- function(column) {
    lev1 <- levels(column)[1]
    column <- as.character(column)
    column <- ifelse(column == lev1, -1, 1)
    column[is.na(column)] <- 0
    column
}

train_num[grepl("^Q" , names(train_num))] <- lapply(train_num[grepl("^Q" , names(train_num))], numericizer)
test_num[grepl("^Q" , names(test_num))] <- lapply(test_num[grepl("^Q" , names(test_num))], numericizer)

## Try imputed YOB but also mean(YOB)
train_num$YOB <- imp_train$YOB
test_num$YOB <- imp_test$YOB
test_num$Income <- imp_test$Income
test_num$HouseholdStatus <- imp_test$HouseholdStatus
test_num$EducationLevel <- imp_test$EducationLevel
test_num$Gender <- imp_test$Gender

```

```{r, echo = FALSE}

num_CART <- train(Party ~.-USER_ID, data = train_num, method = "rpart")

prp(num_CART$finalModel)

num_CART_pred <- predict.train(num_CART, newdata = test_num)

numCART_submission <- data.frame(USER_ID = testing$USER_ID, 
                              Predictions = num_CART_pred)

write.csv(numCART_submission, "submissions/num_CART.csv", row.names=FALSE)

num_RF <-train(Party ~.-USER_ID, data = train_num, method = "rf",
                trControl = trainControl(method="cv", number=5),
                preProcess = c("scale", "center"),
                prox=TRUE, allowParallel=TRUE)

num_RF_pred <- predict.train(num_RF, newdata = test_num)

numRF_submission <- data.frame(USER_ID = testing$USER_ID, 
                              Predictions = num_RF_pred)

write.csv(numRF_submission, "submissions/num_RF.csv", row.names=FALSE)

num_GLM<- glm(Party~.-USER_ID, train_num, family = binomial)
summary(num_GLM)

baseGLM <- glm(Party~.-USER_ID, training, family = binomial)
summary(baseGLM)

num_GLM_pred <- predict(num_GLM, newdata = train_num)

numGLM_submission <- data.frame(USER_ID = testing$USER_ID, 
                              Predictions = num_GLM_pred)

write.csv(numRF_submission, "submissions/num_GLM.csv", row.names=FALSE)

```


```{r, echo = FALSE}

```


```{r, echo=FALSE, cache= TRUE}

noAnsRF <- rf(Party~.-USER_ID , data = trainNoAns)

```

```{r, echo = FALSE}
# should probably do this as part of data cleaning
sort(training$YOB[(2016-training$YOB) >100]) + 100

clean_train <- training
clean_test <- testing

clean_train$YOB[(2016 - clean_train$YOB) >100 & !is.na(clean_train$YOB)] <- clean_train$YOB[
    (2016 - clean_train$YOB) >100 & !is.na(clean_train$YOB)]+100
clean_test$YOB[(2016 - clean_test$YOB) >100 & !is.na(clean_test$YOB)] <- clean_test$YOB[
    (2016 - clean_test$YOB) >100 & !is.na(clean_test$YOB)]+100

clean_train$YOB[(clean_train$YOB) == 2039 & !is.na(clean_test$YOB)] <- 1939

hist(2016-clean_train$YOB, breaks = 20)

sort(unique(clean_train$YOB))

age_bucket <- function(yob){
    age <- 2016 - yob
    if(is.na(age)) NA
    else if(age <= 20) "Under 21"
    else if (age > 20 & age <=30) "21 - 30"
    else if (age > 30 & age <=40) "31 - 40"
    else if (age > 40 & age <=50) "41 - 50"
    else if (age > 50 & age <=60) "51 - 60"
    else "Over 60"
}

clean_train$age_bucket <- sapply(clean_train$YOB, age_bucket)
clean_test$age_bucket <- sapply(clean_test$YOB, age_bucket)


```

## Todo

Control for YOB (scale)
Reapply previous things 

```{r results table, echo=FALSE}
c("CART", 0.61207, "Basic CART without doing anything to the data.")
c("noResp_RF", 0.62069, "Random Forest with NA = noResponse and NA YOB = mean YOB")
c("imp_CART", 0.63649)
c("imp_RF",0.62069)
c("num_RF", 0.61207)
c("num_CART", 0.59626)
c("num_GLM", 0.61207)
```


